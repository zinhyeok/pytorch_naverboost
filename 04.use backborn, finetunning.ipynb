{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.save(): 학습 결과를 저장하자\n",
    "- 학습 결과를 저장하는 방법에는 크게 두 가지가 있다.\n",
    "    - 첫 번째는 모델의 파라미터를 저장하는 것이다.\n",
    "    - 두 번째는 모델의 형태를 저장하는 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoints \n",
    "- 학습의 중간 결과를 저장하여 최선의 결과를 선택\n",
    "- earlystopping 기법 사용시 이전 학습의 결과물을 저장\n",
    "- loss, epoc, mertric을 함께 저장하여 확인\n",
    "- torch.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint란 모델을 저장한 하나의 분기점을 말합니다. Checkpoint는 왜 필요할까요?\n",
    "\n",
    "일반적인 경우로 모델을 학습시킬 때에 우리는 시간이 얼마나 걸릴지, 또 얼만큼을 진행할지 확신하지 못합니다. 이때에 Checkpoint를 여러 개 만들어 두면 1개의 학습을 진행하면서 다른 실험을 병행할 수도 있고, 필요하다면 책을 조금 더 읽을 수도 있습니다.\n",
    "\n",
    "Checkpoint를 활용할 수 있는 대표적인 예시를 들어보겠습니다.\n",
    "\n",
    "학습을 진행하다가 최적의 상태가 된다고 판단되는 경우\n",
    "다양한 모델의 분기를 만들어서 모델들의 집단 지성을 이용하고 싶은 경우\n",
    "학습 도중에 컴퓨터에 이상이 생겨서 학습이 중단되는 경우\n",
    "이 밖에도 다양한 상황에 분기를 만들어 활용하고 모델 학습의 안정성을 높이는 것이 Checkpoint의 목표입니다. 앞서 배운 모델 저장 방식을 그대로 활용해 적절한 분기 조건을 정하고, torch.save를 실행시키는 식으로 코드를 작성하면 됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earlystopping\n",
    "\n",
    "Earlystopping이란, 모델이 학습을 거듭하는 과정 중 최적이라고 판단되는 지점에서 학습을 조기 종료하는 기법입니다. 최적의 상태를 판단하기 위해 모델의 지표를 모니터링하여 꾸준히 좋은 지표가 유지되는지를 고려합니다.\n",
    "\n",
    "예를 들어, 모델이 전체 데이터를 3번쯤 학습한 뒤 현재 학습 데이터에 대한 정확도가 90% 가 되었다고 가정해 봅시다. 다음 4번째, 5번째를 거듭하는데 학습이 진행될수록, 오히려 정확도가 88%, 85%로 떨어지는 경우를 발견하게 됩니다. 이럴 경우 5번째에서 Earlystopping을 수행하면서 학습을 조기 종료할 수 있게 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model & Transfer Learning\n",
    "\n",
    "- Pre-trained 모델이란, 사전에 학습된 모델을 의미합니다. 그럼 이런 Pre-trained 기법은 왜 사용하는 것 일까요?\n",
    "\n",
    "- 가장 핵심적인 이유는 유사한 문제를 학습한 모델로부터 좋은 초기 값을 얻기 위해서 입니다(이미 대량의 데이터로 만들어진 데이터 모델을 사용하자)\n",
    "\n",
    "- 쉬운 이해를 위해 이미지 데이터를 예시로 들어보겠습니다.\n",
    "이미지를 분류하기 위해 모델 A를 사용하고자 합니다. 모델 A의 파라미터는 제법 커서 많은 정보들을 담을 수 있습니다. 하지만 지금 가지고 있는 데이터는 100~200장 정도로 작은 데이터셋이라, 이 데이터로 모든 파라미터를 학습시킨다면 좋은 결과를 얻기가 어려울 것 같네요. 이럴 경우 커다란 데이터 셋으로 이미 사전에 학습되어 있는 Pre-trained 모델을 불러와 가지고 있는 데이터를 추가로 학습시켜 문제를 해결 하게 됩니다.\n",
    "\n",
    "이미 큰 데이터로 사전 학습하여 다양한 정보가 반영된 모델에, 현재 풀고자 하는 문제 데이터인 200장 정도의 이미지를 학습시켜 문제를 더 잘 해결 하도록 하는 것이지요. 이 과정을 Transfer learning 이라고 합니다. 말그대로 기존에 학습된 파라미터를 새로운 문제에 \"전이\" 시키도록 학습하게 됩니다.\n",
    "\n",
    "이런 사전 학습 모델의 직관적인 이해는 \"모델이 일반적인 상황의 데이터를 충분히 학습했다면, 그에 대한 정보를 잘 가지고 있을 것이고, 풀고자 하는 문제의 특성이 유사하다면 적은 수의 데이터로 전체 네트워크를 처음부터 학습시키는 것 보다 좋은 성능을 낼 것\"으로 기대한다는 점 입니다.\n",
    "\n",
    "### Freezing\n",
    "\n",
    "- Freezing 기법은 Transfer learning을 진행할 때에 Pre-trained 모델의 학습 파라미터들을 update 하지 않고 \"얼려둔\" 상태로 학습하는 것을 말합니다. \n",
    "- 일부 layer만 update, 나머지는 frozen, 변경되지 않게 한다\n",
    "- 이 방식을 통해서 기존의 학습 파라미터를 통해 나오게 되는 결과 값에 추가 파라미터를 부착하여 그 부분만 update 되도록 하는 기법입니다. 이를 통해 필요한 만큼의 파라미터를 효율적으로 update하여 연산속도를 올리고 빠른 결과를 내는 것이 목적입니다.\n",
    "\n",
    " -앞서 Auto Grad를 배울 때, Tensor에 사용했던 requires_grad를 기억하시나요? 해당 옵션이 false면 파라미터를 update하지 않는 상태로 바뀌게 되겠죠. 해당 방식을 통해서 freezing을 구현하게 됩니다.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=True).to(device)\n",
    "class MyNewNet(nn.Module):\n",
    "def __init__(self):\n",
    "    super(MyNewNet, self).__init__()\n",
    "    self.vgg19 = models.vgg19(pretrained=True)\n",
    "    self.linear_layers = nn.Linear(1000, 1)\n",
    "# Defining the forward pass\n",
    "def forward(self, x):\n",
    "    x = self.vgg19(x)\n",
    "    return self.linear_layers(x)\n",
    "\n",
    "#freeze\n",
    "for param in my_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in my_model.linear_layers.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 하이퍼 파라미터 튜닝이란?\n",
    "\n",
    "모델을 학습하는 과정에 있어 사람이 임의로 지정해주는 값들이 있는데, 이를 하이퍼 파라미터라 합니다. 그 예로는 learning rate, layer의 수, optimzer의 종류가 있습니다.\n",
    "\n",
    "하이퍼 파라미터를 어떤 값으로 지정하느냐에 따라 모델 성능에 차이가 있기 때문에, 주로 기본적인 성능이 보장된 이후에 하이퍼 파라미터의 수정을 조금씩 진행하는데요. 이를 통해 모델의 성능을 끌어올리는 과정이며 이를 하이퍼 파라미터 튜닝이라고 합니다. 하이퍼 파라미터 튜닝 방법은 대표적으로 두가지가 있습니다.\n",
    "\n",
    "첫번째 방법론은 Grid Search가 있습니다. 하이퍼 파라미터 각각에 탐색할 값들을 지정하고 모든 경우의 수를 실험해보는 방법론을 말합니다. 예를 들어, 하이퍼파라미터는 Batch사이즈, learning rate이 있고 각각 Batch 사이즈는 [32, 64, 128], Learning rate은 [0.1, 0.01, 0.001] 을 탐색한다고 하면, 3X3=9번의 실험을 통해 최적의 조합을 찾습니다. \n",
    "\n",
    "두번째 방법론은 Random search입니다. Grid search와 다른 점은 탐색할 값들을 지정하지 않고 범위만 지정한 다음 그 안에서 각각 하이퍼파라미터 값을 랜덤하게 뽑아서 최적의 조합을 찾는 방법론입니다.\n",
    "\n",
    "이외에도 두 방법론을 같이 사용해서 Random Search를 통해 성능이 잘나오는 구간을 확인하고 Grid search를 통해 값들을 꼼꼼히 확인하는 방법도 있습니다. \n",
    "\n",
    " \n",
    "\n",
    "2) Ray란? \n",
    "\n",
    "Ray는 머신러닝/딥러닝을 할 때 분산 처리를 효과적으로 처리할 수 있도록 도와주는 라이브러리입니다. 머신러닝/딥러닝 모델 학습 전반 뿐 아니라 강화학습, 모델 서빙에서도 활용할 수 있습니다. Ray tune을 통해 Hyperparameter tuning을 위한 다양한 모듈을 제공하고 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) OOM(Out Of Memory)란?\n",
    "\n",
    "우리가 사용하는 GPU도 또한 memory 크기가 정해져 있습니다. 'Out of Memory'는 사용하고 있는 GPU의 메모리 이상으로 할당하면서 발생하는 오류를 말합니다. 딥러닝 모델링 학습시 자주 만나는 오류 중 하나입니다. 모델의 사이즈를 줄이지 않으면서 OOM 오류를 해결하는 방법엔 어떤 것이 있을까요?\n",
    "\n",
    "우선 Batch size를 줄이는 방법이 있습니다. Batch Size를 줄여서 GPU에 할당되는 메모리를 줄여 OOM을 방지 할 수 있습니다.\n",
    "\n",
    "둘째로는 훈련과정에서 낭비되는 memory를 줄이는 방법이 있습니다. torch.cuda.empty_cache()를 통해서 사용되지 않는 cache를 정리할 수 있습니다. trainning loop에서 불필요한 변수를 적절히 del 명령어를 통해 삭제를 하거나 python 기본 객체로 변환하여 처리를 할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
